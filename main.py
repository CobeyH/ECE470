# -*- coding: utf-8 -*-
"""ECE 470 Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YvLUO-sRl9zvoD3Gopa3TuhANIVAwLYu

# ECE 470: House Price Predictions

Cobey Hollier V00893715 \
Kutay Cinar V00886983 \
Chris Dunn V00897180 \
Emmanuel Ayodele V00849004

##Pre Work
"""

# Basic Stuff
import sys
import torch # PyTorch 
import math
import numpy as np 
import pandas as pd 
import sklearn as sk
import tensorflow as tf

# Plotting
import seaborn as sns
from scipy import stats
from matplotlib import pyplot as plt

# Neural Networks
from keras.optimizers import SGD
from keras import regularizers
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
import torchvision.transforms as transforms 
from torch.utils.data.sampler import SubsetRandomSampler # Sampler 

# Trees
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn import ensemble
from sklearn.datasets import load_iris
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score

# Data Management
from torchvision import datasets  
from pandas import read_csv
from google.colab import files

print('-'*40)
print ('Python version: {}'.format(sys.version))

"""ECE 470 Project

To upload files into the session, go the the files tab on the left side screen, click Upload to session storage, and upload the files to use
"""

train = pd.read_csv('/content/train.csv')

"""Data cleaning"""

# Get dummy data
dummifiedDataframe = pd.get_dummies(train)
# Split data into test and validation
trainDf, validationDf = train_test_split(dummifiedDataframe, test_size=0.15, shuffle=True)
labelFeature = "SalePrice"

# Split the data and label into different dataframes
trainRealSalePrice = trainDf[labelFeature]
trainData = trainDf.drop([labelFeature], axis=1)
validationRealSalePrice = validationDf[labelFeature]
validationData = validationDf.drop([labelFeature], axis=1)

"""## Heat Map"""

# Heat map for showing correlation between features
plt.figure(figsize=(15, 13))
sns.heatmap(train.corr())

"""##Decision tree"""

decisionTreeRegressor = tree.DecisionTreeRegressor(criterion="mse", random_state=0, max_depth=10)

# train tree
decisionTree = decisionTreeRegressor.fit(trainData, trainRealSalePrice)

# predict values
validationPredictions = decisionTree.predict(validationData)
trainPredictions = decisionTree.predict(trainData)

# Print Final Results
print('Decision Tree Results:')
print("Training error:", round(mean_squared_error(trainRealSalePrice, trainPredictions, squared=False), 1))
print("Validation error:", round(mean_squared_error(validationRealSalePrice, validationPredictions, squared=False), 1))
print("Maximum tree depth:", decisionTree.tree_.max_depth)
print("Score:", round(decisionTree.score(validationData, validationRealSalePrice)*100,5), "%")

scores = []

# Code to determine which max depth is best
for i in range(1,16):

    decisionTreeRegressor = tree.DecisionTreeRegressor(criterion="mse", random_state=0, max_depth=i)

    # train tree
    decisionTree = decisionTreeRegressor.fit(trainData, trainRealSalePrice)

    # predict values
    validationPredictions = decisionTree.predict(validationData)
    trainPredictions = decisionTree.predict(trainData)

    scores.append(round(decisionTree.score(validationData, validationRealSalePrice)*100,5))

print(scores)

Depths = np.arange(15) + 1

plt.figure(figsize=(10, 10))

rank = [int((max(scores)-elem)*len(df)*0.75/(max(scores)+1)) for elem in scores] 
pal = sns.color_palette("Reds_r",len(scores))
sns.set(style="whitegrid", color_codes=True)

sns.scatterplot(x=Depths, y=scores, hue=scores, palette=pal, legend=False)

"""Plotting Graphs"""

# Plot first 20 samples predictions and real values for the decision tree.
plt.figure(figsize=(15, 10))

# Make x and y labels bigger
plt.rc('xtick', labelsize=15) 
plt.rc('ytick', labelsize=15)

size = 20
X = np.arange(size) + 1
plt.xticks(np.arange(min(X), max(X)+1, 1.0))

# Plot bars
plt.bar(X + 0.00, validationRealSalePrice[:size], color = 'b', width = 0.25)
plt.bar(X + 0.25, validationPredictions[:size], color = 'g', width = 0.25)

# Add legend
plt.legend(labels=['Real house price', 'Estimated sale price'], prop={'size': 16})

plt.show()

"""##Random Forest"""

RandomForestRegressor = ensemble.RandomForestRegressor(n_estimators=500, max_depth=15)

# train tree
randomForest = RandomForestRegressor.fit(trainData, trainRealSalePrice)

# predict values
trainForestPrediction = randomForest.predict(trainData)
validationForestPrediction = randomForest.predict(validationData)

# Print Results
print('RandomForest Results:')
print("Training error:", round(mean_squared_error(trainRealSalePrice, trainForestPrediction, squared=False), 1))
print("Validation error:", round(mean_squared_error(validationRealSalePrice, validationForestPrediction, squared=False), 1))
print("Score:", round(randomForest.score(validationData, validationRealSalePrice),5)*100, "%")

"""##Gradient Boosted Trees"""

from sklearn.ensemble import GradientBoostingRegressor

GradientBoostingRegressor = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.015, max_depth=6, random_state=0)
# train tree
gradientBoosted = GradientBoostingRegressor.fit(trainData, trainRealSalePrice)

# predict values
trainGBTPrediction = gradientBoosted.predict(trainData)
validationGBTPrediction = gradientBoosted.predict(validationData)

# Print Results
print("Training error:", round(mean_squared_error(trainRealSalePrice, trainGBTPrediction, squared=False), 1))
print("Validation error:", round(mean_squared_error(validationRealSalePrice, validationGBTPrediction, squared=False), 1))
print("Score:", round(gradientBoosted.score(validationData, validationRealSalePrice),5)*100, "%")

"""##K-Nearest Neighbour"""

# K-Nearest Neighbour didn't make it into the final report due to it producing very poor results. However, the code is still here in case you are curious.
from sklearn.neighbors import KNeighborsRegressor
neigh = KNeighborsRegressor(n_neighbors=15)
# Train
nearestNeigh = neigh.fit(trainData, trainRealSalePrice)

# Predict values
validationNearestNeigh = neigh.predict(validationData)

# Print Results
print("Validation error:", round(mean_squared_error(validationRealSalePrice, validationNearestNeigh, squared=False), 1))
print("Score:", round(nearestNeigh.score(validationData, validationRealSalePrice),5)*100, "%")

"""##Neural Network

"""

# Create data for neural network to use
trainDf, validationDf = train_test_split(dummifiedDataframe, test_size=0.15, shuffle=True)

# Convert dummified data to the correct format
dummified_np = dummifiedDataframe.to_numpy()
dummified_tf = tf.convert_to_tensor(dummified_np)

# Seperate label from dataframes
trainRealSalePrice = trainDf[labelFeature]
trainData = trainDf.drop([labelFeature], axis=1)
validationRealSalePrice = validationDf[labelFeature]
validationData = validationDf.drop([labelFeature], axis=1)

# Convert training data to correct format
trainData_tf = tf.convert_to_tensor(trainData.to_numpy())
trainRealSalePrice_tf = tf.convert_to_tensor(trainRealSalePrice.to_numpy())

# Create network structure
kerasNetwork = Sequential()
kerasNetwork.add(Dense(128, activation='sigmoid', input_shape=(305,)))
kerasNetwork.add(Dense(1, activation='relu'))
optim = SGD(learning_rate=0.001)
# Put together network
kerasNetwork.compile(optimizer=optim, loss='mean_squared_error', metrics=['accuracy'])

# Train Network
kerasNetwork.fit(trainData, trainRealSalePrice, epochs=5)

# split into input (X) and output (Y) variables
X = trainData
Y = trainRealSalePrice
# define base model
def build_model():
	# create model
  model = Sequential()
  model.add(Dense(305, input_dim=305, kernel_initializer='normal', activation='relu'))
  model.add(Dense(305, kernel_initializer='normal', activation='relu'))
  model.add(Dense(1, kernel_initializer='normal', activation='relu'))
	# Compile model
  model.compile(loss='mean_squared_error', optimizer='adam')
  return model
model1 = build_model()
model1.fit(trainData, trainRealSalePrice, epochs=5000)
# model2 = baseline_model()
# model2.fit(trainData, trainRealSalePrice, epochs=500)

results1 = model1.evaluate(validationData, validationRealSalePrice)
# results2 = model2.evaluate(validationData, validationRealSalePrice)
print(math.sqrt(results1))

"""Fooling around with some tuning"""

# Create model
model = Sequential()
model.add(Dense(305, input_dim=305, kernel_initializer='normal', activation='relu'))
model.add(Dense(100, kernel_initializer='normal', activation='relu'))
model.add(Dense(10, kernel_initializer='normal', activation='relu'))
model.add(Dense(1, kernel_initializer='normal', activation='relu'))
# Compile model
model.compile(loss='mean_squared_error', optimizer='adam')
# Fit model
model.fit(trainData, trainRealSalePrice, epochs=50)
# Results
results1 = model.evaluate(validationData, validationRealSalePrice)
print('3 layer', math.sqrt(results1))

# # Create model
# model = Sequential()
# model.add(Dense(305, input_dim=305, kernel_initializer='normal', activation='relu'))
# model.add(Dense(10, kernel_initializer='normal', activation='relu'))
# model.add(Dense(1, kernel_initializer='normal', activation='relu'))
# # Compile model
# model.compile(loss='mean_squared_error', optimizer='adam')
# # Fit model
# model.fit(trainData, trainRealSalePrice, epochs=50)
# # Results
# results1 = model.evaluate(validationData, validationRealSalePrice)
# print('2 layer', math.sqrt(results1))